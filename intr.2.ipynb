{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mssKc2Ycax-y"
      },
      "outputs": [],
      "source": [
        "# Optimizers :-\n",
        "\n",
        "(1). Stochastic Gradient Descent(SGD):\n",
        "\n",
        "SGD is one of the simplest optimizers. It updates the parameters in the direction of the negative\n",
        "gradient of the loss function with respect to the current mini-batch of training data.However,\n",
        "SGD has a limitation of converging slowly and can get stuck in local minima.\n",
        "\n",
        "(2). Momentum :\n",
        "\n",
        "The moment optimizer builds upon SGD by adding a momentum term.It accumulates a fraction of the\n",
        "previous gradients to determine the direction of the update.This helps to accelerate convergence\n",
        "and navigate through flat regions and local minima.\n",
        "\n",
        "(3). Nesterov Accelerated Gradient(NAG):\n",
        "\n",
        "NAG is an improvement over the momentum optiizer. It calculates the gradient not only based on\n",
        "the current parameters but aslo using an estimate of the future parameters. By looking ahead,\n",
        "NAG allows the optimizer to better anticipate the momentum's effect and adjust its update accordingly\n",
        "\n",
        "(4). AdaGrad:\n",
        "\n",
        "AdaGrad adapts the learning rate for each parameter based on the historical gradients.It increase the learning\n",
        "rate for infrequent features and decreases it for frequent ones. This makes AdaGrad well-suited\n",
        "for sparse data but can cause the learning rate to became too small overtime.\n",
        "\n",
        "(5). RMSprop:\n",
        "\n",
        "RMSprop addresses the diminishing learning rate issue of AdaGrad by introducing an exponentially decaying\n",
        "average of squared gradients.By"
      ]
    }
  ]
}